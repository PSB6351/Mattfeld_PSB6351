{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You first need to set you directory structure\n",
    "# and collect the behavioral files for the localizer and the\n",
    "# study task separately.  Given that each task will be modeled\n",
    "# separately treat them separately.\n",
    "sid = [\"001\"]\n",
    "proj_dir = \"/Users/chloehampson/Desktop/classes/psb6351/\"\n",
    "# work_dir = '/scratch/madlab/Mattfeld_PSB6351/amattfel/'\n",
    "behav_dir = op.join(proj_dir, f\"raw/sub-{sid[0]}/ses-01/behav\")\n",
    "\n",
    "# Get a list of my study task json and nifti converted files\n",
    "behav_tsv = sorted(glob(behav_dir + \"/*.tsv\"))\n",
    "behav_loc_tsv = behav_tsv[0:2]\n",
    "behav_task_tsv = behav_tsv[2:7]\n",
    "# use os.path.join or Pathlib to define location of files\n",
    "# use glob and sort to grab relevant files...I would separately handle localizer and task .tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/sub-001_task-study_run-1_events.tsv',\n",
       " '/Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/sub-001_task-study_run-2_events.tsv',\n",
       " '/Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/sub-001_task-study_run-3_events.tsv',\n",
       " '/Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/sub-001_task-study_run-4_events.tsv']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_task_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved .1D files:\n",
      "Face Run 1: /Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/loc_face_run_1.1D\n",
      "Scene Run 1: /Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/loc_scene_run_1.1D\n",
      "Saved .1D files:\n",
      "Face Run 2: /Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/loc_face_run_2.1D\n",
      "Scene Run 2: /Users/chloehampson/Desktop/classes/psb6351/raw/sub-001/ses-01/behav/loc_scene_run_2.1D\n"
     ]
    }
   ],
   "source": [
    "# In this cell I'm going to first work on the localizer task\n",
    "\n",
    "# Define variables that can distinguish between runs and have\n",
    "# onset times and their duration embedded.  I used dictionaries\n",
    "# This will need to be accomplished for each type of stimulus or\n",
    "# or regressor you want to isolate onset times for\n",
    "\n",
    "# Iterate over your text files for the localizer task.\n",
    "# The variable curr_behav_file will be a string variable with\n",
    "# the full path to the separate runs of the localizer task. idx is a counter\n",
    "# used for indexing.\n",
    "for idx, curr_behav_file in enumerate(behav_loc_tsv):\n",
    "    # This is where I defined the keys in my dictionaries that were created above\n",
    "    # and defined the output associated with each key as an empty list\n",
    "    # example --> name_of_dictionary_defined_above[f'run{SOMETHING}'] = []\n",
    "\n",
    "    # I'm using the pandas function read_csv to read in the log files\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "\n",
    "    # Given that the localizer task is a block design you don't need\n",
    "    # the onset time of each stimulus.  You just need the first and then\n",
    "    # the total duration of all stimuli to create your blocked time\n",
    "    # to convolve with your hemodynamic response.  I used temporary lists\n",
    "    # to grab all onset times and then picked the first...you can choose\n",
    "    # another way.\n",
    "\n",
    "    # iterating over ????? here...i is counter for indexing\n",
    "    # What column header would you want to iterate over?  Why?\n",
    "    tmp_face_onset = []\n",
    "    tmp_scene_onset = []\n",
    "    tmp_math_onset = []\n",
    "    tmp_face_math = []  \n",
    "    tmp_scene_math = []\n",
    "\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['trial_type']):\n",
    "        if (\n",
    "            curr_trial_type == \"face\" and i % 20 == 0\n",
    "        ):  # Check if the trial type is 'face' and i is a multiple of 20\n",
    "            tmp_face_onset.append(\n",
    "                curr_behav_data[\"onset\"][i]\n",
    "            )  # Append onset time for 'face' every 20 trials\n",
    "        elif (\n",
    "            curr_trial_type == \"scence\" \n",
    "        ):  \n",
    "            if i % 20 == 10: # Check for 'scene' at every 20 trials\n",
    "                tmp_scene_onset.append(\n",
    "                    curr_behav_data[\"onset\"][i]\n",
    "                )  # Append onset time for 'scene' every 20 trials\n",
    "        elif (\n",
    "            curr_trial_type == \"math\"\n",
    "        ):  # Check for 'math' at every 20 trials\n",
    "            # Add to face_math or scene_math based on the original index\n",
    "            if i % 10 == 0:  # Collect math onsets at every 20th index starting from 0\n",
    "                tmp_math_onset.append(\n",
    "                    curr_behav_data[\"onset\"][i]\n",
    "                )  # Append onset time for 'math' at the 0th index and every 20th after\n",
    "\n",
    "                # Add to face_math or scene_math based on the original index\n",
    "                if len(tmp_face_math) <= len(tmp_scene_math):\n",
    "                    tmp_face_math.append(curr_behav_data[\"onset\"][i])  # Even index for face_math\n",
    "                else:\n",
    "                    tmp_scene_math.append(curr_behav_data[\"onset\"][i])  # Odd index for scene_math\n",
    "\n",
    "    # Convert lists to NumPy arrays for calculations\n",
    "    face_onset_array = np.array(tmp_face_onset)\n",
    "    face_math_array = np.array(tmp_face_math)\n",
    "    scene_onset_array = np.array(tmp_scene_onset)  # Add for scene onsets\n",
    "    scene_math_array = np.array(tmp_scene_math)  # Add for scene math onsets\n",
    "\n",
    "    # Ensure the lengths are the same for subtraction for face\n",
    "    min_length_face = min(len(face_onset_array), len(face_math_array))\n",
    "    face_duration = (\n",
    "        face_math_array[:min_length_face] - face_onset_array[:min_length_face]\n",
    "    )\n",
    "\n",
    "    # Ensure the lengths are the same for subtraction for scene\n",
    "    min_length_scene = min(len(scene_onset_array), len(scene_math_array))\n",
    "    scene_duration = (\n",
    "        scene_math_array[:min_length_scene] - scene_onset_array[:min_length_scene]\n",
    "    )\n",
    "\n",
    "    # Combine onsets and durations into the \"onset:duration\" format for saving\n",
    "    face_run_data = np.array(\n",
    "        [f\"{face_onset_array[i]}:{face_duration[i]}\" for i in range(min_length_face)]\n",
    "    )\n",
    "    scene_run_data = np.array(\n",
    "        [f\"{scene_onset_array[i]}:{scene_duration[i]}\" for i in range(min_length_scene)]\n",
    "    )\n",
    "\n",
    "    # Save to .1D files in behav_dir\n",
    "    with open(f\"{behav_dir}/loc_face_run_{idx + 1}.1D\", \"w\") as face_file:\n",
    "        face_file.write(\", \".join(face_run_data))\n",
    "\n",
    "    with open(f\"{behav_dir}/loc_scene_run_{idx + 1}.1D\", \"w\") as scene_file:\n",
    "        scene_file.write(\", \".join(scene_run_data))\n",
    "\n",
    "    # Print paths for confirmation\n",
    "    print(\"Saved .1D files:\")\n",
    "    print(f\"Face Run {idx + 1}: {behav_dir}/loc_face_run_{idx + 1}.1D\")\n",
    "    print(f\"Scene Run {idx + 1}: {behav_dir}/loc_scene_run_{idx + 1}.1D\")\n",
    "\n",
    "    # here I am using the first trial type when it becomes math and the\n",
    "    # face onset list variable is ???? elements long (just exited a face block)\n",
    "    # to assign the first element of the tmp_face_onset list variable to the\n",
    "    # dictionary that I created earlier.\n",
    "    # the format of AFNI stimulus timing files is the following:\n",
    "    #\n",
    "    # onset_time:duration (e.g., 6.3:25, 12.7:25, 22.5:25) --> A block started at 6.3, 12.7, and 22.5 seconds\n",
    "    # into the experiment and each block lasted for 25 seconds in length...make sure these numbers match your\n",
    "    # experimental design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_remain_events_onset_times = {f\"run{run_num}\": [] for run_num in range(1, 5)}\n",
    "task_facefixb4_bl_onset_times = {f\"run{run_num}\": [] for run_num in range(1, 5)}\n",
    "\n",
    "# Dictionaries to store onset times based on accuracy\n",
    "task_remain_events_correct_onset_times = {\n",
    "    f\"run{run_num}\": [] for run_num in range(1, 5)\n",
    "}\n",
    "task_remain_events_incorrect_onset_times = {\n",
    "    f\"run{run_num}\": [] for run_num in range(1, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, curr_behav_file in enumerate(behav_task_tsv):\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep=\"\\t\")\n",
    "\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data[\"trial_type\"]):\n",
    "        tmp_fix_onset = curr_behav_data[\"onset\"][i]\n",
    "\n",
    "        # Check for fixed association trials followed by conditional trials\n",
    "        if \"face\" in curr_trial_type or \"scene\" in curr_trial_type:\n",
    "            if i > 0:\n",
    "                if (\n",
    "                    \"face\" in curr_behav_data[\"trial_type\"][i - 1]\n",
    "                    or \"scene\" in curr_behav_data[\"trial_type\"][i - 1]\n",
    "                ):\n",
    "                    if curr_behav_data[\"acc\"][i] == 1:\n",
    "                        task_remain_events_correct_onset_times[f\"run{idx+1}\"].append(\n",
    "                            tmp_fix_onset\n",
    "                        )\n",
    "                    elif curr_behav_data[\"acc\"][i] == 0:\n",
    "                        task_remain_events_incorrect_onset_times[f\"run{idx+1}\"].append(\n",
    "                            tmp_fix_onset\n",
    "                        )\n",
    "\n",
    "        # Fixed association trials preceding correct/incorrect conditional trials\n",
    "        elif \"conditional\" in curr_trial_type:\n",
    "            if i > 0 and \"face\" in curr_behav_data[\"trial_type\"][i - 1]:\n",
    "                if curr_behav_data[\"acc\"][i] == 1:\n",
    "                    task_remain_events_correct_onset_times[f\"run{idx+1}\"].append(\n",
    "                        tmp_fix_onset\n",
    "                    )\n",
    "                elif curr_behav_data[\"acc\"][i] == 0:\n",
    "                    task_remain_events_incorrect_onset_times[f\"run{idx+1}\"].append(\n",
    "                        tmp_fix_onset\n",
    "                    )\n",
    "\n",
    "        # Face/scene trials preceding baseline trials for MVPA\n",
    "        elif curr_trial_type == \"baseline\" and (\n",
    "            \"face\" in curr_behav_data[\"trial_type\"][i - 1]\n",
    "            or \"scene\" in curr_behav_data[\"trial_type\"][i - 1]\n",
    "        ):\n",
    "            task_facefixb4_bl_onset_times[f\"run{idx+1}\"].append(tmp_fix_onset)\n",
    "\n",
    "        ###almost works but there are some little problems that need to fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_run in [\"run1\", \"run2\", \"run3\", \"run4\"]:\n",
    "    for onset_dict in [\n",
    "        task_remain_events_onset_times,\n",
    "        task_facefixb4_bl_onset_times,\n",
    "        task_remain_events_correct_onset_times, #doesnt work\n",
    "        task_remain_events_incorrect_onset_times, #doesnt work\n",
    "    ]:\n",
    "        if len(onset_dict[curr_run]) == 0:\n",
    "            onset_dict[curr_run].append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to write onset times to files\n",
    "def save_onsets(onset_dict, condition_name):\n",
    "    with open(os.path.join(behav_dir, f\"{condition_name}_onsets.txt\"), \"w\") as f:\n",
    "        for curr_run in [\"run1\", \"run2\", \"run3\", \"run4\"]:\n",
    "            # Join onset times for each run, separated by commas, and write to file\n",
    "            f.write(\", \".join(map(str, onset_dict[curr_run])) + \"\\n\")\n",
    "\n",
    "\n",
    "# Save onset times for each condition\n",
    "save_onsets(task_remain_events_onset_times, \"task_remain_events\")\n",
    "save_onsets(task_facefixb4_bl_onset_times, \"task_facefixb4_bl\")\n",
    "save_onsets(task_remain_events_correct_onset_times, \"task_remain_events_correct\") #doesnt work\n",
    "save_onsets(task_remain_events_incorrect_onset_times, \"task_remain_events_incorrect\") #doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for idx, curr_behav_file in enumerate(behav_task_tsv):\\n    # I set the run key for each condition of interest\\n    # Again if you opt not to use the dictionaries as I did\\n    # you'll need to utilize a different way. HINT you should\\n    # update the key for as many events of interest you are\\n    # attempting to isolate\\n    \\n    # I read in the current study run behavioral file\\n    # NOTE: sometimes it is not good practice to use the same\\n    # variable names across cells. It may execute but erroneously\\n    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\\n    \\n    # I iterate now over the contents of the run specific data.\\n    # Again...you'll want to isolate the column header that lets\\n    # you assess the event or trial types of interest\\n    for i, curr_trial_type in enumerate(curr_behav_data['trial_type']):\\n        # I am evaluating whether or not the current trail type was a \\n        # fixed association that had a conditional trial that followed with a face\\n        # or a scene\\n        if 'face' in curr_trial_type or 'scene' in curr_trial_type:\\n            # if it was either of those grab that onset\\n            tmp_fix_onset = curr_behav_data['onset'][i]\\n            # if this is not our first trial (i = counter > 0) - remember python is 0-based\\n            if i > 0:\\n                # evaluate whether or not the LAST TRIAL (i-1) was a scene or face fix trial\\n                # grab the current onset time and assign it to the remaining events.\\n                # In the analysis that I am interested in pursuing I want to separate these\\n                # trials from trials where the fix face and scence trials are followed either\\n                # by a conditional trial or by a baseline trial\\n                \\n                if 'face' in curr_behav_data['trial_type'][i-1] or 'scene' in curr_behav_data['trial_type'][i-1]:\\n                    # Check the accuracy for this trial\\n                    if curr_behav_data['acc'][i] == 1:\\n                        # Correct trial, add onset to correct dictionary\\n                        task_remain_events_correct_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\\n                    elif curr_behav_data['acc'][i] == 0:\\n                        # Incorrect trial, add onset to incorrect dictionary\\n                        task_remain_events_incorrect_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\\n                        \\n        # Here I am evaluating whether or not the current trial type is a conditional trial \\n        # that was responded to correctly\\n        elif # grab the onset times for fixed association trials before correct and incorrect conditional trials\\n             # this will require multiple elif statements.  You'll have to assess what the curr_trial_type is\\n             # and whether or not that trial was correct or incorrect.  You'll then input that 'tmp_fix_onset'\\n             # variable that you held onto when you evaluated the above trial types\\n        # Now I am doing the same thing for trials that precede the perceptual baseline trials\\n        # These trials will be used for the MVPA anlaysis that is planned.\\n        elif curr_trial_type == 'baseline' and '?????' in curr_behav_data['trial_type'][i-1]:\\n                task_facefixb4_bl_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\\n        \\n            \\n# Given that we're setting things up to analyze in AFNI\\n# you can't have runs that don't have any events in them....or\\n# you can but you can't have an empty row...thus here I am checking\\n# to see if the runs are empty and if they are adding a filler (-1)\\nfor curr_run in ['run1', 'run2', 'run3', 'run4']:\\n    if len(NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run]) == 0:\\n        NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run].append(-1)\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar to above I am creating empty dictionary variables\n",
    "# for each of the events that I am interested in.\n",
    "# I will then insert run keys to separate the timing files \n",
    "# for the events of interest and their specific runs.\n",
    "\n",
    "\n",
    "# Here I am iterating over the study behavior files.  There should be\n",
    "# 4 of them.\n",
    "'''for idx, curr_behav_file in enumerate(behav_task_tsv):\n",
    "    # I set the run key for each condition of interest\n",
    "    # Again if you opt not to use the dictionaries as I did\n",
    "    # you'll need to utilize a different way. HINT you should\n",
    "    # update the key for as many events of interest you are\n",
    "    # attempting to isolate\n",
    "    \n",
    "    # I read in the current study run behavioral file\n",
    "    # NOTE: sometimes it is not good practice to use the same\n",
    "    # variable names across cells. It may execute but erroneously\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # I iterate now over the contents of the run specific data.\n",
    "    # Again...you'll want to isolate the column header that lets\n",
    "    # you assess the event or trial types of interest\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['trial_type']):\n",
    "        # I am evaluating whether or not the current trail type was a \n",
    "        # fixed association that had a conditional trial that followed with a face\n",
    "        # or a scene\n",
    "        if 'face' in curr_trial_type or 'scene' in curr_trial_type:\n",
    "            # if it was either of those grab that onset\n",
    "            tmp_fix_onset = curr_behav_data['onset'][i]\n",
    "            # if this is not our first trial (i = counter > 0) - remember python is 0-based\n",
    "            if i > 0:\n",
    "                # evaluate whether or not the LAST TRIAL (i-1) was a scene or face fix trial\n",
    "                # grab the current onset time and assign it to the remaining events.\n",
    "                # In the analysis that I am interested in pursuing I want to separate these\n",
    "                # trials from trials where the fix face and scence trials are followed either\n",
    "                # by a conditional trial or by a baseline trial\n",
    "                \n",
    "                if 'face' in curr_behav_data['trial_type'][i-1] or 'scene' in curr_behav_data['trial_type'][i-1]:\n",
    "                    # Check the accuracy for this trial\n",
    "                    if curr_behav_data['acc'][i] == 1:\n",
    "                        # Correct trial, add onset to correct dictionary\n",
    "                        task_remain_events_correct_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\n",
    "                    elif curr_behav_data['acc'][i] == 0:\n",
    "                        # Incorrect trial, add onset to incorrect dictionary\n",
    "                        task_remain_events_incorrect_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\n",
    "                        \n",
    "        # Here I am evaluating whether or not the current trial type is a conditional trial \n",
    "        # that was responded to correctly\n",
    "        elif # grab the onset times for fixed association trials before correct and incorrect conditional trials\n",
    "             # this will require multiple elif statements.  You'll have to assess what the curr_trial_type is\n",
    "             # and whether or not that trial was correct or incorrect.  You'll then input that 'tmp_fix_onset'\n",
    "             # variable that you held onto when you evaluated the above trial types\n",
    "        # Now I am doing the same thing for trials that precede the perceptual baseline trials\n",
    "        # These trials will be used for the MVPA anlaysis that is planned.\n",
    "        elif curr_trial_type == 'baseline' and '?????' in curr_behav_data['trial_type'][i-1]:\n",
    "                task_facefixb4_bl_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\n",
    "        \n",
    "            \n",
    "# Given that we're setting things up to analyze in AFNI\n",
    "# you can't have runs that don't have any events in them....or\n",
    "# you can but you can't have an empty row...thus here I am checking\n",
    "# to see if the runs are empty and if they are adding a filler (-1)\n",
    "for curr_run in ['run1', 'run2', 'run3', 'run4']:\n",
    "    if len(NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run]) == 0:\n",
    "        NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run].append(-1)'''\n",
    "    # the code above would have to be repeated for each variable that is \n",
    "    # capturing a stimulus condition of interest. THis is not the most\n",
    "    # effecient way of coding and can be improved upon.\n",
    "\n",
    "# Reformatting as before to save as a text file with no square brackets\n",
    "# use what you did for the localizer task here. If you use the same way as before\n",
    "# or that I used you'll have to have a separate line for each run of each condition\n",
    "# This is not a great way to code and could be improved upon\n",
    "\n",
    "# Creating and checking to see if the directory exists for where you want to save your text files\n",
    "# use the way you did this for the localizer task\n",
    "\n",
    "\n",
    "# creating my separate ev files with runs written to each line\n",
    "# use the way you did this before. HINT if you keep the same way as before\n",
    "# and how I did it you'll need a separate block of code for each regressor (aka stimulus condition)\n",
    "# you are trying to model and a separate line within each block for each run\n",
    "# In the end it should look something like the following in one file:\n",
    "#\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# \n",
    "# In the example above this stimulus had the exact same onset times across each runs\n",
    "# your output should in format look like this but your onset times should not be the\n",
    "# same across runs!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is required to run the following 3dDeconvolve calls in the subsequent cells\n",
    "# Here I am collecting the motion related regressor files that were created for each run\n",
    "# separately, concatenating them and saving the output.\n",
    "\n",
    "# the below directory structure will change to match how your data are organized\n",
    "motion_dir = op.join(proj_dir, 'derivatives/preproc/sub-001/motion1stAFNI')\n",
    "os.makedirs(motion_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_motion_files = sorted(glob(proj_dir + motion_dir + \"/st*/*loc*bold.1D\"))\n",
    "loc_mot_dict = {}\n",
    "for mot_i, curr_loc_motion_file in enumerate(loc_motion_files):\n",
    "    loc_mot_dict[f\"run{mot_i + 1}\"] = np.genfromtxt(curr_loc_motion_file)\n",
    "\n",
    "allruns_loc_motion_data = np.concatenate((loc_mot_dict[\"run1\"], loc_mot_dict[\"run2\"]))\n",
    "np.savetxt(proj_dir + motion_dir + \"/allruns_loc_mot_data.1D\", allruns_loc_motion_data)\n",
    "\n",
    "task_motion_files = sorted(glob(proj_dir + motion_dir + \"/st*/*study*bold.1D\"))\n",
    "task_mot_dict = {}\n",
    "for mot_i, curr_task_motion_file in enumerate(task_motion_files):\n",
    "    task_mot_dict[f\"run{mot_i + 1}\"] = np.genfromtxt(curr_task_motion_file)\n",
    "\n",
    "allruns_task_motion_data = np.concatenate(\n",
    "    (\n",
    "        task_mot_dict[\"run1\"],\n",
    "        task_mot_dict[\"run2\"],\n",
    "        task_mot_dict[\"run3\"],\n",
    "        task_mot_dict[\"run4\"],\n",
    "    )\n",
    ")\n",
    "np.savetxt(\n",
    "    proj_dir + motion_dir + \"/allruns_task_mot_data.1D\", allruns_task_motion_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Given that we are running this with no data (see -nodata flag) we can run a quick bash\n",
    "# command in the cell by using line above.  The matrices that are created and image\n",
    "# will be created in the directory where this jupyter notebook is running. In my case.\n",
    "# /home/data/madlab/Mattfeld_PSB6351/mattfeld_2020/code\n",
    "\n",
    "3dDeconvolve -nodata 608 1.76 \\\n",
    "-concat '1D: 0 304' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_loc_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 2 \\\n",
    "-stim_times_AM1 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_face_evs.1D \"dmBLOCK(1)\" -stim_label 1 faces \\\n",
    "-stim_times_AM1 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_scene_evs.1D \"dmBLOCK(1)\" -stim_label 2 scenes \\\n",
    "-x1D X.loc.xmat.1D -xjpeg X.loc.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "3dDeconvolve -nodata 1420 1.76 \\\n",
    "-concat '1D: 0 355 710 1065' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_task_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 5 \\\n",
    "-stim_times 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_c_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 1 fx_b4_c_cond \\\n",
    "-stim_times 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_ic_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 2 fx_b4_ic_cond \\\n",
    "-stim_times 3 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/events_remain_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 3 all_remain \\\n",
    "-stim_times 4 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/facefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 4 facefx_b4_bl \\\n",
    "-stim_times 5 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/scenefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 5 scenefx_b4_bl \\\n",
    "-x1D X.task.xmat.1D -xjpeg X.task.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_task = Image.open(os.path.join(os.getcwd(), 'X.task.jpg'))\n",
    "im_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_loc = Image.open(os.path.join(os.getcwd(), 'X.loc.jpg'))\n",
    "im_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
