{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localizer files: ['/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-loc_run-1_events.tsv', '/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-loc_run-2_events.tsv']\n",
      "Study task files: ['/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-study_run-1_events.tsv', '/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-study_run-2_events.tsv', '/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-study_run-3_events.tsv', '/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav/sub-021_task-study_run-4_events.tsv']\n"
     ]
    }
   ],
   "source": [
    "# You first need to set you directory structure\n",
    "# and collect the behavioral files for the localizer and the\n",
    "# study task separately.  Given that each task will be modeled\n",
    "# separately treat them separately.\n",
    "# use os.path.join or Pathlib to define location of files\n",
    "# use glob and sort to grab relevant files...I would separately handle localizer and task .tsv files\n",
    "\n",
    "# My directory on my local computer\n",
    "proj_dir = '/Users/alisha/Desktop/CogNeuro-Imaging-Methods/Mattfeld_PSB6351/behav'\n",
    "\n",
    "# Used glob and * wild card element like previous assignment to locate localizer and study task files\n",
    "localizer_files = sorted(glob(os.path.join(proj_dir, '*_task-loc_*_events.tsv')))\n",
    "study_task_files = sorted(glob(os.path.join(proj_dir, '*_task-study_*_events.tsv')))\n",
    "\n",
    "# Print files to make sure the path is correct\n",
    "print(\"Localizer files:\", localizer_files)\n",
    "print(\"Study task files:\", study_task_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face onset times: {'run1': ['6.01005580311:25', '81.0102012303:25', '156.010947896:25', '231.012253435:25', '306.02929695:25', '381.030623221:25', '456.031369286:25'], 'run2': ['6.010115296:25', '81.0108769855:25', '156.012685209:25', '231.012760026:25', '306.014053246:25', '381.014985602:25', '456.015892418:25']}\n",
      "Scence onset times: {'run1': ['43.5105377558:25', '118.509990601:25', '193.51066846:25', '268.528655448:25', '343.529607034:25', '418.530665587:25', '493.531335934:25'], 'run2': ['43.5103616808:25', '118.510910338:25', '193.512537979:25', '268.513247087:25', '343.514315555:25', '418.515630709:25', '493.516578389:25']}\n",
      "Math onset times: {'run1': ['31.0005110982:25', '32.2505222155:25', '33.5003924128:25', '34.7504756427:25', '36.0005450512:25', '37.2506934829:25', '38.5008551354:25', '39.7507088069:25', '41.001029107:25', '42.2506505159:25', '68.5006691449:25', '69.7509666095:25', '71.0006451074:25', '72.250683868:25', '73.5007514735:25', '74.7506994923:25', '76.0007139149:25', '77.2508052575:25', '78.5007292388:25', '79.7508235861:25', '106.00090261:25', '107.251066966:25', '108.500985539:25', '109.750985539:25', '111.001057051:25', '112.251060957:25', '113.50105675:25', '114.75099906:25', '116.001035417:25', '117.251271886:25', '143.501365332:25', '144.751295022:25', '146.001540806:25', '147.25133228:25', '148.501445858:25', '149.751480412:25', '151.001440149:25', '152.251407398:25', '153.501443755:25', '154.751520975:25', '181.001732205:25', '182.251803116:25', '183.501683229:25', '184.751852393:25', '186.001775773:25', '187.251801012:25', '188.501739717:25', '189.75182535:25', '191.001813332:25', '192.251828956:25', '218.502212956:25', '219.752176599:25', '221.002134834:25', '222.252105688:25', '223.502224374:25', '224.752219566:25', '226.002222571:25', '227.252257125:25', '228.502197031:25', '229.752353575:25', '256.019203288:25', '257.269286518:25', '258.519413015:25', '259.769267588:25', '261.019367344:25', '262.269357729:25', '263.519253767:25', '264.769399494:25', '266.01936464:25', '267.269288321:25', '293.519686442:25', '294.769674724:25', '296.019689147:25', '297.269705071:25', '298.519779287:25', '299.769799118:25', '301.019747438:25', '302.269798217:25', '303.519818048:25', '304.769751043:25', '331.020132639:25', '332.270307212:25', '333.520238705:25', '334.770284376:25', '336.020369409:25', '337.270297296:25', '338.520316526:25', '339.770253127:25', '341.020337259:25', '342.270358893:25', '368.520640432:25', '369.770722761:25', '371.020663869:25', '372.270691512:25', '373.520841146:25', '374.770787962:25', '376.020890723:25', '377.270877202:25', '378.520829427:25', '379.770852563:25', '406.021265708:25', '407.271229652:25', '408.521246479:25', '409.771284037:25', '411.021271117:25', '412.271290948:25', '413.521382892:25', '414.771331511:25', '416.021376882:25', '417.271306272:25', '443.521654816:25', '444.771752168:25', '446.021671943:25', '447.271845013:25', '448.521723924:25', '449.771829689:25', '451.02188167:25', '452.27181767:25', '453.521823379:25', '454.771847417:25', '481.022245839:25', '482.272225708:25', '483.522271679:25', '484.772304731:25', '486.022294515:25', '487.272316449:25', '488.522344694:25', '489.772473895:25', '491.022364224:25', '492.272610609:25', '518.522728693:25', '519.772820937:25', '521.022760543:25', '522.272752731:25', '523.522921895:25', '524.772935115:25', '526.02279059:25', '527.273024355:25', '528.522908975:25', '529.772960956:25'], 'run2': ['31.000790835:25', '32.250785126:25', '33.5009017081:25', '34.75081968:25', '36.0008470227:25', '37.2510053699:25', '38.5009569945:25', '39.7508953982:25', '41.0009831353:25', '42.2510206939:25', '68.5013088438:25', '69.7513554166:25', '71.0016492755:25', '72.2515960925:25', '73.5017249937:25', '74.7513989847:25', '76.0014960362:25', '77.2514122053:25', '78.5013524119:25', '79.7515152663:25', '106.001872825:25', '107.25182535:25', '108.501895059:25', '109.751915792:25', '111.001865914:25', '112.251999623:25', '113.502224674:25', '114.751912487:25', '116.002015547:25', '117.251966571:25', '143.502270045:25', '144.752354176:25', '146.002320524:25', '147.252399547:25', '148.50238843:25', '149.752361387:25', '151.002528448:25', '152.25246535:25', '153.502332843:25', '154.752570514:25', '181.002783246:25', '182.25289472:25', '183.502949406:25', '184.752907941:25', '186.002897425:25', '187.252858364:25', '188.503216523:25', '189.752893519:25', '191.002986964:25', '192.25306088:25', '218.503271509:25', '219.753312373:25', '221.003398908:25', '222.253324391:25', '223.503382081:25', '224.753398006:25', '226.003465311:25', '227.253366157:25', '228.503384786:25', '229.753399509:25', '256.003813555:25', '257.25401487:25', '258.504108616:25', '259.754209874:25', '261.004038607:25', '262.254025987:25', '263.50404732:25', '264.753983621:25', '266.004023583:25', '267.254111921:25', '293.504422306:25', '294.754364916:25', '296.004475489:25', '297.254476991:25', '298.50443132:25', '299.754430118:25', '301.004461667:25', '302.254551508:25', '303.504556015:25', '304.754530775:25', '331.005022343:25', '332.254964653:25', '333.504977573:25', '334.75500161:25', '336.00508424:25', '337.25509746:25', '338.504970362:25', '339.755061704:25', '341.005087845:25', '342.255123301:25', '368.505483563:25', '369.75558422:25', '371.005496483:25', '372.255540953:25', '373.505538549:25', '374.755505197:25', '376.00557731:25', '377.255523225:25', '378.505648821:25', '379.755578211:25', '406.006005478:25', '407.256099225:25', '408.506025009:25', '409.756043638:25', '411.006007281:25', '412.256095018:25', '413.506210098:25', '414.756231732:25', '416.00612807:25', '417.256168332:25', '443.506614229:25', '444.75653791:25', '446.006631656:25', '447.25653791:25', '448.506628651:25', '449.75656405:25', '451.006656294:25', '452.256679731:25', '453.506645177:25', '454.756569759:25', '481.007053515:25', '482.257033684:25', '483.507249421:25', '484.757126228:25', '486.007341064:25', '487.2572359:25', '488.507059224:25', '489.757124425:25', '491.007367505:25', '492.257296895:25', '518.507615993:25', '519.75769081:25', '521.007708538:25', '522.257737082:25', '523.507755411:25', '524.757824819:25', '526.00776112:25', '527.257710941:25', '528.507677589:25', '529.757803786:25']}\n",
      "Scence Run 1 Data: 43.5105377558:25, 118.509990601:25, 193.51066846:25, 268.528655448:25, 343.529607034:25, 418.530665587:25, 493.531335934:25\n",
      "Scence Run 2 Data: 43.5103616808:25, 118.510910338:25, 193.512537979:25, 268.513247087:25, 343.514315555:25, 418.515630709:25, 493.516578389:25\n",
      "Face Run 1 Data: 6.01005580311:25, 81.0102012303:25, 156.010947896:25, 231.012253435:25, 306.02929695:25, 381.030623221:25, 456.031369286:25\n",
      "Face Run 2 Data: 6.010115296:25, 81.0108769855:25, 156.012685209:25, 231.012760026:25, 306.014053246:25, 381.014985602:25, 456.015892418:25\n",
      "Math Run 1 Data: 31.0005110982:25, 32.2505222155:25, 33.5003924128:25, 34.7504756427:25, 36.0005450512:25, 37.2506934829:25, 38.5008551354:25, 39.7507088069:25, 41.001029107:25, 42.2506505159:25, 68.5006691449:25, 69.7509666095:25, 71.0006451074:25, 72.250683868:25, 73.5007514735:25, 74.7506994923:25, 76.0007139149:25, 77.2508052575:25, 78.5007292388:25, 79.7508235861:25, 106.00090261:25, 107.251066966:25, 108.500985539:25, 109.750985539:25, 111.001057051:25, 112.251060957:25, 113.50105675:25, 114.75099906:25, 116.001035417:25, 117.251271886:25, 143.501365332:25, 144.751295022:25, 146.001540806:25, 147.25133228:25, 148.501445858:25, 149.751480412:25, 151.001440149:25, 152.251407398:25, 153.501443755:25, 154.751520975:25, 181.001732205:25, 182.251803116:25, 183.501683229:25, 184.751852393:25, 186.001775773:25, 187.251801012:25, 188.501739717:25, 189.75182535:25, 191.001813332:25, 192.251828956:25, 218.502212956:25, 219.752176599:25, 221.002134834:25, 222.252105688:25, 223.502224374:25, 224.752219566:25, 226.002222571:25, 227.252257125:25, 228.502197031:25, 229.752353575:25, 256.019203288:25, 257.269286518:25, 258.519413015:25, 259.769267588:25, 261.019367344:25, 262.269357729:25, 263.519253767:25, 264.769399494:25, 266.01936464:25, 267.269288321:25, 293.519686442:25, 294.769674724:25, 296.019689147:25, 297.269705071:25, 298.519779287:25, 299.769799118:25, 301.019747438:25, 302.269798217:25, 303.519818048:25, 304.769751043:25, 331.020132639:25, 332.270307212:25, 333.520238705:25, 334.770284376:25, 336.020369409:25, 337.270297296:25, 338.520316526:25, 339.770253127:25, 341.020337259:25, 342.270358893:25, 368.520640432:25, 369.770722761:25, 371.020663869:25, 372.270691512:25, 373.520841146:25, 374.770787962:25, 376.020890723:25, 377.270877202:25, 378.520829427:25, 379.770852563:25, 406.021265708:25, 407.271229652:25, 408.521246479:25, 409.771284037:25, 411.021271117:25, 412.271290948:25, 413.521382892:25, 414.771331511:25, 416.021376882:25, 417.271306272:25, 443.521654816:25, 444.771752168:25, 446.021671943:25, 447.271845013:25, 448.521723924:25, 449.771829689:25, 451.02188167:25, 452.27181767:25, 453.521823379:25, 454.771847417:25, 481.022245839:25, 482.272225708:25, 483.522271679:25, 484.772304731:25, 486.022294515:25, 487.272316449:25, 488.522344694:25, 489.772473895:25, 491.022364224:25, 492.272610609:25, 518.522728693:25, 519.772820937:25, 521.022760543:25, 522.272752731:25, 523.522921895:25, 524.772935115:25, 526.02279059:25, 527.273024355:25, 528.522908975:25, 529.772960956:25\n",
      "Math Run 2 Data: 31.000790835:25, 32.250785126:25, 33.5009017081:25, 34.75081968:25, 36.0008470227:25, 37.2510053699:25, 38.5009569945:25, 39.7508953982:25, 41.0009831353:25, 42.2510206939:25, 68.5013088438:25, 69.7513554166:25, 71.0016492755:25, 72.2515960925:25, 73.5017249937:25, 74.7513989847:25, 76.0014960362:25, 77.2514122053:25, 78.5013524119:25, 79.7515152663:25, 106.001872825:25, 107.25182535:25, 108.501895059:25, 109.751915792:25, 111.001865914:25, 112.251999623:25, 113.502224674:25, 114.751912487:25, 116.002015547:25, 117.251966571:25, 143.502270045:25, 144.752354176:25, 146.002320524:25, 147.252399547:25, 148.50238843:25, 149.752361387:25, 151.002528448:25, 152.25246535:25, 153.502332843:25, 154.752570514:25, 181.002783246:25, 182.25289472:25, 183.502949406:25, 184.752907941:25, 186.002897425:25, 187.252858364:25, 188.503216523:25, 189.752893519:25, 191.002986964:25, 192.25306088:25, 218.503271509:25, 219.753312373:25, 221.003398908:25, 222.253324391:25, 223.503382081:25, 224.753398006:25, 226.003465311:25, 227.253366157:25, 228.503384786:25, 229.753399509:25, 256.003813555:25, 257.25401487:25, 258.504108616:25, 259.754209874:25, 261.004038607:25, 262.254025987:25, 263.50404732:25, 264.753983621:25, 266.004023583:25, 267.254111921:25, 293.504422306:25, 294.754364916:25, 296.004475489:25, 297.254476991:25, 298.50443132:25, 299.754430118:25, 301.004461667:25, 302.254551508:25, 303.504556015:25, 304.754530775:25, 331.005022343:25, 332.254964653:25, 333.504977573:25, 334.75500161:25, 336.00508424:25, 337.25509746:25, 338.504970362:25, 339.755061704:25, 341.005087845:25, 342.255123301:25, 368.505483563:25, 369.75558422:25, 371.005496483:25, 372.255540953:25, 373.505538549:25, 374.755505197:25, 376.00557731:25, 377.255523225:25, 378.505648821:25, 379.755578211:25, 406.006005478:25, 407.256099225:25, 408.506025009:25, 409.756043638:25, 411.006007281:25, 412.256095018:25, 413.506210098:25, 414.756231732:25, 416.00612807:25, 417.256168332:25, 443.506614229:25, 444.75653791:25, 446.006631656:25, 447.25653791:25, 448.506628651:25, 449.75656405:25, 451.006656294:25, 452.256679731:25, 453.506645177:25, 454.756569759:25, 481.007053515:25, 482.257033684:25, 483.507249421:25, 484.757126228:25, 486.007341064:25, 487.2572359:25, 488.507059224:25, 489.757124425:25, 491.007367505:25, 492.257296895:25, 518.507615993:25, 519.75769081:25, 521.007708538:25, 522.257737082:25, 523.507755411:25, 524.757824819:25, 526.00776112:25, 527.257710941:25, 528.507677589:25, 529.757803786:25\n"
     ]
    }
   ],
   "source": [
    "# Define variables that can distinguish between runs and have \n",
    "# onset times and their duration embedded. I used dictionaries.\n",
    "# This will need to be accomplished for each type of stimulus or\n",
    "# regressor you want to isolate onset times for.\n",
    "\n",
    "# Prepare dictionaries to hold the onset times for each condition\n",
    "loc_face_onset_times = {'run1': [], 'run2': []}\n",
    "loc_scence_onset_times = {'run1': [], 'run2': []}\n",
    "loc_math_onset_times = {'run1': [], 'run2': []}\n",
    "\n",
    "# The localizer_files variable is already defined in the first part of the code, using glob to find relevant files.\n",
    "# Here we proceed with this dynamically generated list.\n",
    "for idx, curr_behav_file in enumerate(localizer_files):  \n",
    "    # Read in the log files\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # Temporary lists to grab all onset times for each condition\n",
    "    tmp_face_onset = []\n",
    "    tmp_scence_onset = []\n",
    "    tmp_math_onset = []\n",
    "    \n",
    "    # Iterating over trial types\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['trial_type']):  \n",
    "        if curr_trial_type == 'face':  # Checking for face stimuli\n",
    "            tmp_face_onset.append(curr_behav_data['onset'][i])\n",
    "        elif curr_trial_type == 'scence':  # Checking for scence stimuli\n",
    "            tmp_scence_onset.append(curr_behav_data['onset'][i])\n",
    "        elif curr_trial_type == 'math':  # Checking for math stimuli\n",
    "            if len(tmp_face_onset) > 0:  \n",
    "                loc_face_onset_times[f'run{idx + 1}'].append(f\"{tmp_face_onset[0]}:25\")\n",
    "                tmp_face_onset = []  # Reset temporary variable for next face block\n",
    "            \n",
    "            if len(tmp_scence_onset) > 0:  \n",
    "                loc_scence_onset_times[f'run{idx + 1}'].append(f\"{tmp_scence_onset[0]}:25\")\n",
    "                tmp_scence_onset = []  # Reset temporary variable for next scence block\n",
    "                \n",
    "            if len(tmp_math_onset) == 0:  \n",
    "                tmp_math_onset.append(curr_behav_data['onset'][i])  \n",
    "            \n",
    "            if len(tmp_math_onset) > 0:  \n",
    "                loc_math_onset_times[f'run{idx + 1}'].append(f\"{tmp_math_onset[0]}:25\")\n",
    "                tmp_math_onset = []  # Reset for the next math block\n",
    "\n",
    "# Convert lists to comma-separated strings for output\n",
    "loc_scence_run1_data = \", \".join(map(str, loc_scence_onset_times['run1']))\n",
    "loc_scence_run2_data = \", \".join(map(str, loc_scence_onset_times['run2']))\n",
    "loc_face_run1_data = \", \".join(map(str, loc_face_onset_times['run1']))\n",
    "loc_face_run2_data = \", \".join(map(str, loc_face_onset_times['run2']))\n",
    "loc_math_run1_data = \", \".join(map(str, loc_math_onset_times['run1']))  # Correct\n",
    "loc_math_run2_data = \", \".join(map(str, loc_math_onset_times['run2']))\n",
    "\n",
    "# Define the sink directory where to save the timing files\n",
    "evs_sink_dir = os.path.join(proj_dir, 'EV_files_output')\n",
    "if not os.path.isdir(evs_sink_dir):\n",
    "    os.makedirs(evs_sink_dir)\n",
    "\n",
    "# Define and save the files for the localizer task\n",
    "loc_scence_evs_file = 'loc_scence_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_scence_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_scence_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_scence_run2_data}\\n'])\n",
    "\n",
    "loc_face_evs_file = 'loc_face_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_face_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_face_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_face_run2_data}\\n'])\n",
    "\n",
    "loc_math_evs_file = 'loc_math_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_math_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_math_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_math_run2_data}\\n'])\n",
    "\n",
    "# Print statements to check the output\n",
    "print(\"Face onset times:\", loc_face_onset_times)\n",
    "print(\"Scence onset times:\", loc_scence_onset_times)\n",
    "print(\"Math onset times:\", loc_math_onset_times)\n",
    "print(\"Scence Run 1 Data:\", loc_scence_run1_data)\n",
    "print(\"Scence Run 2 Data:\", loc_scence_run2_data)\n",
    "print(\"Face Run 1 Data:\", loc_face_run1_data)\n",
    "print(\"Face Run 2 Data:\", loc_face_run2_data)\n",
    "print(\"Math Run 1 Data:\", loc_math_run1_data)\n",
    "print(\"Math Run 2 Data:\", loc_math_run2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I'm going to first work on the localizer task\n",
    "\n",
    "# Define variables that can distinguish between runs and have \n",
    "# onset times and their duration embedded.  I used dictionaries\n",
    "# This will need to be accomplished for each type of stimulus or\n",
    "# or regressor you want to isolate onset times for\n",
    "\n",
    "# Iterate over your text files for the localizer task.\n",
    "# The variable curr_behav_file will be a string variable with\n",
    "# the full path to the separate runs of the localizer task. idx is a counter\n",
    "# used for indexing.\n",
    "for idx, curr_behav_file in enumerate(name_of_variable_for_localizer_files_defined_in_previous_cell):\n",
    "    # This is where I defined the keys in my dictionaries that were created above\n",
    "    # and defined the output associated with each key as an empty list\n",
    "    # example --> name_of_dictionary_defined_above[f'run{SOMETHING}'] = []\n",
    "    \n",
    "    # I'm using the pandas function read_csv to read in the log files\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # Given that the localizer task is a block design you don't need \n",
    "    # the onset time of each stimulus.  You just need the first and then\n",
    "    # the total duration of all stimuli to create your blocked time\n",
    "    # to convolve with your hemodynamic response.  I used temporary lists\n",
    "    # to grab all onset times and then picked the first...you can choose\n",
    "    # another way.\n",
    "    \n",
    "    # iterating over ????? here...i is counter for indexing\n",
    "    # What column header would you want to iterate over?  Why?\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['?????']):\n",
    "        if curr_trial_type == '????': #What would you want to look for in this conditional statement?\n",
    "            # Here I am appending the onset of the stimulus if the current\n",
    "            # trial type is a ??????.\n",
    "            # I used a list append function here to grow my temporary list variable\n",
    "            # reference above face-by-face...you can choose a different way\n",
    "            # example --> name_of_temp_variable.append(curr_behav_data['????'][i]) #What is i representing? \n",
    "        elif curr_trial_type == '?????': #note...scence was misspelled originally\n",
    "            # Here I am appending the onset of the stimulus if the current\n",
    "            # trial type is a scene.\n",
    "            # I used a list append function here to grow my temporary list variable\n",
    "            # reference above face-by-face...you can choose a different way\n",
    "            \n",
    "        # here I am using the first trial type when it becomes math and the \n",
    "        # face onset list variable is ???? elements long (just exited a face block)\n",
    "        # to assign the first element of the tmp_face_onset list variable to the \n",
    "        # dictionary that I created earlier.\n",
    "        # the format of AFNI stimulus timing files is the following:\n",
    "        #\n",
    "        # onset_time:duration (e.g., 6.3:25, 12.7:25, 22.5:25) --> A block started at 6.3, 12.7, and 22.5 seconds\n",
    "        # into the experiment and each block lasted for 25 seconds in length...make sure these numbers match your\n",
    "        # experimental design\n",
    "        elif curr_trial_type == 'math' and len(tmp_face_onset) == ?????:\n",
    "            # This is where I update my run dictionary with the current onset time and duration of \n",
    "            # the block of stimuli of a specific condition....HINT: 0 is the first in a list of elements\n",
    "            # Be sure to reset your temporary variable as you will be now collecting a new set\n",
    "            # of onset times for your new block of stimuli\n",
    "        elif curr_trial_type == 'math' and len(tmp_scene_onset) == ?????:\n",
    "            # Do the same as you did above but for the different stimuli conditions.\n",
    "            \n",
    "# The following code creates a string element that has the square brackets\n",
    "# removed.  This is important for the following steps below.\n",
    "# I'm leaving this for you...If you opt to not use dictionaries like I did then\n",
    "# you'll have to find a different way.\n",
    "loc_scene_run1_data = \", \".join(map(str, loc_scene_onset_times['run1']))\n",
    "loc_scene_run2_data = \", \".join(map(str, loc_scene_onset_times['run2']))\n",
    "loc_face_run1_data = \", \".join(map(str, loc_face_onset_times['run1']))\n",
    "loc_face_run2_data = \", \".join(map(str, loc_face_onset_times['run2']))\n",
    "\n",
    "# Here I am defining the sink directory where I would like to save the timing files\n",
    "# HINT you shouldn't save these text files in teh same directory where your code resides\n",
    "# you can also you Pathlib to create this and make the direcotry if it doesn't exist\n",
    "evs_sink_dir = os.path.join(proj_dir, 'where', 'do', 'you', 'want', 'these', 'files', 'saved')\n",
    "# I check to see if the directory exists.  If it doesn't I create it.\n",
    "if not os.path.isdir(evs_sink_dir):\n",
    "    os.makedirs(evs_sink_dir)\n",
    "    \n",
    "# below I am defining the file names for the localizer (loc) face and scene evs.\n",
    "# each run is captured on a separate line with the multiple onsets within a run\n",
    "# captured on a single line\n",
    "# I'm leaving this for you...again this may constrain the way you do things above\n",
    "# but it gives you a hint of how to output data into a AFNI style format\n",
    "# your output should eventually look like the following:\n",
    "#\n",
    "# 6.3:25, 30.7:25, 76.5:25, 137.3:25, 225.1:25\n",
    "# 6.3:25, 30.7:25, 76.5:25, 137.3:25, 225.1:25\n",
    "#\n",
    "# The first line refers to run1 and the second line refers to run2\n",
    "# in this case the onset times were identical for this particular condition across runs\n",
    "loc_scene_evs_file = 'loc_scene_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_scene_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_scene_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_scene_run2_data}\\n'])\n",
    "loc_face_evs_file = 'loc_face_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_face_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_face_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_face_run2_data}\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to above I am creating empty dictionary variables\n",
    "# for each of the events that I am interested in.\n",
    "# I will then insert run keys to separate the timing files \n",
    "# for the events of interest and their specific runs.\n",
    "\n",
    "# Here I am iterating over the study behavior files.  There should be\n",
    "# 4 of them.\n",
    "for idx, curr_behav_file in enumerate(name_of_variable_for_study_files_defined_in_previous_cell):\n",
    "    # I set the run key for each condition of interest\n",
    "    # Again if you opt not to use the dictionaries as I did\n",
    "    # you'll need to utilize a different way. HINT you should\n",
    "    # update the key for as many events of interest you are\n",
    "    # attempting to isolate\n",
    "    \n",
    "    # I read in the current study run behavioral file\n",
    "    # NOTE: sometimes it is not good practice to use the same\n",
    "    # variable names across cells. It may execute but erroneously\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # I iterate now over the contents of the run specific data.\n",
    "    # Again...you'll want to isolate the column header that lets\n",
    "    # you assess the event or trial types of interest\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['??????']):\n",
    "        # I am evaluating whether or not the current trail type was a \n",
    "        # fixed association that had a conditional trial that followed with a face\n",
    "        # or a scene\n",
    "        if '??????' in curr_trial_type or '?????' in curr_trial_type:\n",
    "            # if it was either of those grab that onset\n",
    "            tmp_fix_onset = GRAB THE ONSET WITH THE APPROPRIATE CODE\n",
    "            # if this is not our first trial (i = counter > 0) - remember python is 0-based\n",
    "            if i > 0:\n",
    "                # evaluate whether or not the LAST TRIAL (i-1) was a scene or face fix trial\n",
    "                # grab the current onset time and assign it to the remaining events.\n",
    "                # In the analysis that I am interested in pursuing I want to separate these\n",
    "                # trials from trials where the fix face and scence trials are followed either\n",
    "                # by a conditional trial or by a baseline trial\n",
    "                \n",
    "                if '?????' in curr_behav_data['trial_type'][i-1] or '??????' in curr_behav_data['trial_type'][i-1]:\n",
    "                    task_remain_events_onset_times[f'run{idx+1}'].append(curr_behav_data['onset'][i])\n",
    "        # Here I am evaluating whether or not the current trial type is a conditional trial \n",
    "        # that was responded to correctly\n",
    "        elif # grab the onset times for fixed association trials before correct and incorrect conditional trials\n",
    "             # this will require multiple elif statements.  You'll have to assess what the curr_trial_type is\n",
    "             # and whether or not that trial was correct or incorrect.  You'll then input that 'tmp_fix_onset'\n",
    "             # variable that you held onto when you evaluated the above trial types\n",
    "        # Now I am doing the same thing for trials that precede the perceptual baseline trials\n",
    "        # These trials will be used for the MVPA anlaysis that is planned.\n",
    "        elif curr_trial_type == 'baseline' and '?????' in curr_behav_data['trial_type'][i-1]:\n",
    "                task_facefixb4_bl_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\n",
    "        \n",
    "            \n",
    "# Given that we're setting things up to analyze in AFNI\n",
    "# you can't have runs that don't have any events in them....or\n",
    "# you can but you can't have an empty row...thus here I am checking\n",
    "# to see if the runs are empty and if they are adding a filler (-1)\n",
    "for curr_run in ['run1', 'run2', 'run3', 'run4']:\n",
    "    if len(NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run]) == 0:\n",
    "        NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run].append(-1)\n",
    "    # the code above would have to be repeated for each variable that is \n",
    "    # capturing a stimulus condition of interest. THis is not the most\n",
    "    # effecient way of coding and can be improved upon.\n",
    "\n",
    "# Reformatting as before to save as a text file with no square brackets\n",
    "# use what you did for the localizer task here. If you use the same way as before\n",
    "# or that I used you'll have to have a separate line for each run of each condition\n",
    "# This is not a great way to code and could be improved upon\n",
    "\n",
    "# Creating and checking to see if the directory exists for where you want to save your text files\n",
    "# use the way you did this for the localizer task\n",
    "\n",
    "\n",
    "# creating my separate ev files with runs written to each line\n",
    "# use the way you did this before. HINT if you keep the same way as before\n",
    "# and how I did it you'll need a separate block of code for each regressor (aka stimulus condition)\n",
    "# you are trying to model and a separate line within each block for each run\n",
    "# In the end it should look something like the following in one file:\n",
    "#\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# \n",
    "# In the example above this stimulus had the exact same onset times across each runs\n",
    "# your output should in format look like this but your onset times should not be the\n",
    "# same across runs!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is required to run the following 3dDeconvolve calls in the subsequent cells\n",
    "# Here I am collecting the motion related regressor files that were created for each run\n",
    "# separately, concatenating them and saving the output.\n",
    "\n",
    "# the below directory structure will change to match how your data are organized\n",
    "proj_dir = '/home/amattfel/Mattfeld_PSB6351/'\n",
    "motion_dir = '/derivatives/preproc/sub-021/motion1stAFNI'\n",
    "\n",
    "loc_motion_files = sorted(glob(proj_dir + motion_dir + '/st*/*loc*bold.1D'))\n",
    "loc_mot_dict = {}\n",
    "for mot_i, curr_loc_motion_file in enumerate(loc_motion_files):\n",
    "    loc_mot_dict[f'run{mot_i + 1}'] = np.genfromtxt(curr_loc_motion_file)\n",
    "    \n",
    "allruns_loc_motion_data = np.concatenate((loc_mot_dict['run1'], loc_mot_dict['run2']))\n",
    "np.savetxt(proj_dir + motion_dir + '/allruns_loc_mot_data.1D', allruns_loc_motion_data)\n",
    "\n",
    "task_motion_files = sorted(glob(proj_dir + motion_dir + '/st*/*study*bold.1D'))\n",
    "task_mot_dict = {}\n",
    "for mot_i, curr_task_motion_file in enumerate(task_motion_files):\n",
    "    task_mot_dict[f'run{mot_i + 1}'] = np.genfromtxt(curr_task_motion_file)\n",
    "    \n",
    "allruns_task_motion_data = np.concatenate((task_mot_dict['run1'], task_mot_dict['run2'],\n",
    "                                           task_mot_dict['run3'], task_mot_dict['run4']))\n",
    "np.savetxt(proj_dir + motion_dir + '/allruns_task_mot_data.1D', allruns_task_motion_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Given that we are running this with no data (see -nodata flag) we can run a quick bash\n",
    "# command in the cell by using line above.  The matrices that are created and image\n",
    "# will be created in the directory where this jupyter notebook is running. In my case.\n",
    "# /home/data/madlab/Mattfeld_PSB6351/mattfeld_2020/code\n",
    "\n",
    "3dDeconvolve -nodata 608 1.76 \\\n",
    "-concat '1D: 0 304' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_loc_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 2 \\\n",
    "-stim_times_AM1 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_face_evs.1D \"dmBLOCK(1)\" -stim_label 1 faces \\\n",
    "-stim_times_AM1 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_scene_evs.1D \"dmBLOCK(1)\" -stim_label 2 scenes \\\n",
    "-x1D X.loc.xmat.1D -xjpeg X.loc.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "3dDeconvolve -nodata 1420 1.76 \\\n",
    "-concat '1D: 0 355 710 1065' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_task_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 5 \\\n",
    "-stim_times 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_c_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 1 fx_b4_c_cond \\\n",
    "-stim_times 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_ic_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 2 fx_b4_ic_cond \\\n",
    "-stim_times 3 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/events_remain_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 3 all_remain \\\n",
    "-stim_times 4 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/facefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 4 facefx_b4_bl \\\n",
    "-stim_times 5 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/scenefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 5 scenefx_b4_bl \\\n",
    "-x1D X.task.xmat.1D -xjpeg X.task.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_task = Image.open(os.path.join(os.getcwd(), 'X.task.jpg'))\n",
    "im_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_loc = Image.open(os.path.join(os.getcwd(), 'X.loc.jpg'))\n",
    "im_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
