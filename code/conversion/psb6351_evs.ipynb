{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You first need to set you directory structure\n",
    "# and collect the behavioral files for the localizer and the\n",
    "# study task separately.  Given that each task will be modeled\n",
    "# separately treat them separately.\n",
    "proj_dir = '/your/project/directory/location'\n",
    "# use os.path.join or Pathlib to define location of files\n",
    "# use glob and sort to grab relevant files...I would separately handle localizer and task .tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell I'm going to first work on the localizer task\n",
    "\n",
    "# Define variables that can distinguish between runs and have \n",
    "# onset times and their duration embedded.  I used dictionaries\n",
    "# This will need to be accomplished for each type of stimulus or\n",
    "# or regressor you want to isolate onset times for\n",
    "\n",
    "# Iterate over your text files for the localizer task.\n",
    "# The variable curr_behav_file will be a string variable with\n",
    "# the full path to the separate runs of the localizer task. idx is a counter\n",
    "# used for indexing.\n",
    "for idx, curr_behav_file in enumerate(name_of_variable_for_localizer_files_defined_in_previous_cell):\n",
    "    # This is where I defined the keys in my dictionaries that were created above\n",
    "    # and defined the output associated with each key as an empty list\n",
    "    # example --> name_of_dictionary_defined_above[f'run{SOMETHING}'] = []\n",
    "    \n",
    "    # I'm using the pandas function read_csv to read in the log files\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # Given that the localizer task is a block design you don't need \n",
    "    # the onset time of each stimulus.  You just need the first and then\n",
    "    # the total duration of all stimuli to create your blocked time\n",
    "    # to convolve with your hemodynamic response.  I used temporary lists\n",
    "    # to grab all onset times and then picked the first...you can choose\n",
    "    # another way.\n",
    "    \n",
    "    # iterating over ????? here...i is counter for indexing\n",
    "    # What column header would you want to iterate over?  Why?\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['?????']):\n",
    "        if curr_trial_type == '????': #What would you want to look for in this conditional statement?\n",
    "            # Here I am appending the onset of the stimulus if the current\n",
    "            # trial type is a ??????.\n",
    "            # I used a list append function here to grow my temporary list variable\n",
    "            # reference above face-by-face...you can choose a different way\n",
    "            # example --> name_of_temp_variable.append(curr_behav_data['????'][i]) #What is i representing? \n",
    "        elif curr_trial_type == '?????': #note...scence was misspelled originally\n",
    "            # Here I am appending the onset of the stimulus if the current\n",
    "            # trial type is a scene.\n",
    "            # I used a list append function here to grow my temporary list variable\n",
    "            # reference above face-by-face...you can choose a different way\n",
    "            \n",
    "        # here I am using the first trial type when it becomes math and the \n",
    "        # face onset list variable is ???? elements long (just exited a face block)\n",
    "        # to assign the first element of the tmp_face_onset list variable to the \n",
    "        # dictionary that I created earlier.\n",
    "        # the format of AFNI stimulus timing files is the following:\n",
    "        #\n",
    "        # onset_time:duration (e.g., 6.3:25, 12.7:25, 22.5:25) --> A block started at 6.3, 12.7, and 22.5 seconds\n",
    "        # into the experiment and each block lasted for 25 seconds in length...make sure these numbers match your\n",
    "        # experimental design\n",
    "        elif curr_trial_type == 'math' and len(tmp_face_onset) == ?????:\n",
    "            # This is where I update my run dictionary with the current onset time and duration of \n",
    "            # the block of stimuli of a specific condition....HINT: 0 is the first in a list of elements\n",
    "            # Be sure to reset your temporary variable as you will be now collecting a new set\n",
    "            # of onset times for your new block of stimuli\n",
    "        elif curr_trial_type == 'math' and len(tmp_scene_onset) == ?????:\n",
    "            # Do the same as you did above but for the different stimuli conditions.\n",
    "            \n",
    "# The following code creates a string element that has the square brackets\n",
    "# removed.  This is important for the following steps below.\n",
    "# I'm leaving this for you...If you opt to not use dictionaries like I did then\n",
    "# you'll have to find a different way.\n",
    "loc_scene_run1_data = \", \".join(map(str, loc_scene_onset_times['run1']))\n",
    "loc_scene_run2_data = \", \".join(map(str, loc_scene_onset_times['run2']))\n",
    "loc_face_run1_data = \", \".join(map(str, loc_face_onset_times['run1']))\n",
    "loc_face_run2_data = \", \".join(map(str, loc_face_onset_times['run2']))\n",
    "\n",
    "# Here I am defining the sink directory where I would like to save the timing files\n",
    "# HINT you shouldn't save these text files in teh same directory where your code resides\n",
    "# you can also you Pathlib to create this and make the direcotry if it doesn't exist\n",
    "evs_sink_dir = os.path.join(proj_dir, 'where', 'do', 'you', 'want', 'these', 'files', 'saved')\n",
    "# I check to see if the directory exists.  If it doesn't I create it.\n",
    "if not os.path.isdir(evs_sink_dir):\n",
    "    os.makedirs(evs_sink_dir)\n",
    "    \n",
    "# below I am defining the file names for the localizer (loc) face and scene evs.\n",
    "# each run is captured on a separate line with the multiple onsets within a run\n",
    "# captured on a single line\n",
    "# I'm leaving this for you...again this may constrain the way you do things above\n",
    "# but it gives you a hint of how to output data into a AFNI style format\n",
    "# your output should eventually look like the following:\n",
    "#\n",
    "# 6.3:25, 30.7:25, 76.5:25, 137.3:25, 225.1:25\n",
    "# 6.3:25, 30.7:25, 76.5:25, 137.3:25, 225.1:25\n",
    "#\n",
    "# The first line refers to run1 and the second line refers to run2\n",
    "# in this case the onset times were identical for this particular condition across runs\n",
    "loc_scene_evs_file = 'loc_scene_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_scene_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_scene_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_scene_run2_data}\\n'])\n",
    "loc_face_evs_file = 'loc_face_evs.1D'\n",
    "with open(os.path.join(evs_sink_dir, loc_face_evs_file), 'wt') as fp:\n",
    "    fp.writelines([f'{loc_face_run1_data}\\n'])\n",
    "    fp.writelines([f'{loc_face_run2_data}\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to above I am creating empty dictionary variables\n",
    "# for each of the events that I am interested in.\n",
    "# I will then insert run keys to separate the timing files \n",
    "# for the events of interest and their specific runs.\n",
    "\n",
    "# Here I am iterating over the study behavior files.  There should be\n",
    "# 4 of them.\n",
    "for idx, curr_behav_file in enumerate(name_of_variable_for_study_files_defined_in_previous_cell):\n",
    "    # I set the run key for each condition of interest\n",
    "    # Again if you opt not to use the dictionaries as I did\n",
    "    # you'll need to utilize a different way. HINT you should\n",
    "    # update the key for as many events of interest you are\n",
    "    # attempting to isolate\n",
    "    \n",
    "    # I read in the current study run behavioral file\n",
    "    # NOTE: sometimes it is not good practice to use the same\n",
    "    # variable names across cells. It may execute but erroneously\n",
    "    curr_behav_data = pd.read_csv(curr_behav_file, sep='\\t')\n",
    "    \n",
    "    # I iterate now over the contents of the run specific data.\n",
    "    # Again...you'll want to isolate the column header that lets\n",
    "    # you assess the event or trial types of interest\n",
    "    for i, curr_trial_type in enumerate(curr_behav_data['??????']):\n",
    "        # I am evaluating whether or not the current trail type was a \n",
    "        # fixed association that had a conditional trial that followed with a face\n",
    "        # or a scene\n",
    "        if '??????' in curr_trial_type or '?????' in curr_trial_type:\n",
    "            # if it was either of those grab that onset\n",
    "            tmp_fix_onset = GRAB THE ONSET WITH THE APPROPRIATE CODE\n",
    "            # if this is not our first trial (i = counter > 0) - remember python is 0-based\n",
    "            if i > 0:\n",
    "                # evaluate whether or not the LAST TRIAL (i-1) was a scene or face fix trial\n",
    "                # grab the current onset time and assign it to the remaining events.\n",
    "                # In the analysis that I am interested in pursuing I want to separate these\n",
    "                # trials from trials where the fix face and scence trials are followed either\n",
    "                # by a conditional trial or by a baseline trial\n",
    "                \n",
    "                if '?????' in curr_behav_data['trial_type'][i-1] or '??????' in curr_behav_data['trial_type'][i-1]:\n",
    "                    task_remain_events_onset_times[f'run{idx+1}'].append(curr_behav_data['onset'][i])\n",
    "        # Here I am evaluating whether or not the current trial type is a conditional trial \n",
    "        # that was responded to correctly\n",
    "        elif # grab the onset times for fixed association trials before correct and incorrect conditional trials\n",
    "             # this will require multiple elif statements.  You'll have to assess what the curr_trial_type is\n",
    "             # and whether or not that trial was correct or incorrect.  You'll then input that 'tmp_fix_onset'\n",
    "             # variable that you held onto when you evaluated the above trial types\n",
    "        # Now I am doing the same thing for trials that precede the perceptual baseline trials\n",
    "        # These trials will be used for the MVPA anlaysis that is planned.\n",
    "        elif curr_trial_type == 'baseline' and '?????' in curr_behav_data['trial_type'][i-1]:\n",
    "                task_facefixb4_bl_onset_times[f'run{idx+1}'].append(tmp_fix_onset)\n",
    "        \n",
    "            \n",
    "# Given that we're setting things up to analyze in AFNI\n",
    "# you can't have runs that don't have any events in them....or\n",
    "# you can but you can't have an empty row...thus here I am checking\n",
    "# to see if the runs are empty and if they are adding a filler (-1)\n",
    "for curr_run in ['run1', 'run2', 'run3', 'run4']:\n",
    "    if len(NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run]) == 0:\n",
    "        NAME_OF_DICT_FOR_STIMULUS_CONDITION_OF_INTEREST[curr_run].append(-1)\n",
    "    # the code above would have to be repeated for each variable that is \n",
    "    # capturing a stimulus condition of interest. THis is not the most\n",
    "    # effecient way of coding and can be improved upon.\n",
    "\n",
    "# Reformatting as before to save as a text file with no square brackets\n",
    "# use what you did for the localizer task here. If you use the same way as before\n",
    "# or that I used you'll have to have a separate line for each run of each condition\n",
    "# This is not a great way to code and could be improved upon\n",
    "\n",
    "# Creating and checking to see if the directory exists for where you want to save your text files\n",
    "# use the way you did this for the localizer task\n",
    "\n",
    "\n",
    "# creating my separate ev files with runs written to each line\n",
    "# use the way you did this before. HINT if you keep the same way as before\n",
    "# and how I did it you'll need a separate block of code for each regressor (aka stimulus condition)\n",
    "# you are trying to model and a separate line within each block for each run\n",
    "# In the end it should look something like the following in one file:\n",
    "#\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# 6.3, 30.7, 76.5, 137.3, 225.1\n",
    "# \n",
    "# In the example above this stimulus had the exact same onset times across each runs\n",
    "# your output should in format look like this but your onset times should not be the\n",
    "# same across runs!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is required to run the following 3dDeconvolve calls in the subsequent cells\n",
    "# Here I am collecting the motion related regressor files that were created for each run\n",
    "# separately, concatenating them and saving the output.\n",
    "\n",
    "# the below directory structure will change to match how your data are organized\n",
    "proj_dir = '/home/amattfel/Mattfeld_PSB6351/'\n",
    "motion_dir = '/derivatives/preproc/sub-021/motion1stAFNI'\n",
    "\n",
    "loc_motion_files = sorted(glob(proj_dir + motion_dir + '/st*/*loc*bold.1D'))\n",
    "loc_mot_dict = {}\n",
    "for mot_i, curr_loc_motion_file in enumerate(loc_motion_files):\n",
    "    loc_mot_dict[f'run{mot_i + 1}'] = np.genfromtxt(curr_loc_motion_file)\n",
    "    \n",
    "allruns_loc_motion_data = np.concatenate((loc_mot_dict['run1'], loc_mot_dict['run2']))\n",
    "np.savetxt(proj_dir + motion_dir + '/allruns_loc_mot_data.1D', allruns_loc_motion_data)\n",
    "\n",
    "task_motion_files = sorted(glob(proj_dir + motion_dir + '/st*/*study*bold.1D'))\n",
    "task_mot_dict = {}\n",
    "for mot_i, curr_task_motion_file in enumerate(task_motion_files):\n",
    "    task_mot_dict[f'run{mot_i + 1}'] = np.genfromtxt(curr_task_motion_file)\n",
    "    \n",
    "allruns_task_motion_data = np.concatenate((task_mot_dict['run1'], task_mot_dict['run2'],\n",
    "                                           task_mot_dict['run3'], task_mot_dict['run4']))\n",
    "np.savetxt(proj_dir + motion_dir + '/allruns_task_mot_data.1D', allruns_task_motion_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Given that we are running this with no data (see -nodata flag) we can run a quick bash\n",
    "# command in the cell by using line above.  The matrices that are created and image\n",
    "# will be created in the directory where this jupyter notebook is running. In my case.\n",
    "# /home/data/madlab/Mattfeld_PSB6351/mattfeld_2020/code\n",
    "\n",
    "3dDeconvolve -nodata 608 1.76 \\\n",
    "-concat '1D: 0 304' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_loc_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 2 \\\n",
    "-stim_times_AM1 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_face_evs.1D \"dmBLOCK(1)\" -stim_label 1 faces \\\n",
    "-stim_times_AM1 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/loc_scene_evs.1D \"dmBLOCK(1)\" -stim_label 2 scenes \\\n",
    "-x1D X.loc.xmat.1D -xjpeg X.loc.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "3dDeconvolve -nodata 1420 1.76 \\\n",
    "-concat '1D: 0 355 710 1065' \\\n",
    "-ortvec /home/amattfel/Mattfeld_PSB6351/derivatives/preproc/sub-021/motion1stAFNI/allruns_task_mot_data.1D motion \\\n",
    "-polort A \\\n",
    "-local_times \\\n",
    "-num_stimts 5 \\\n",
    "-stim_times 1 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_c_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 1 fx_b4_c_cond \\\n",
    "-stim_times 2 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/fix_b4_ic_cond_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 2 fx_b4_ic_cond \\\n",
    "-stim_times 3 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/events_remain_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 3 all_remain \\\n",
    "-stim_times 4 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/facefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 4 facefx_b4_bl \\\n",
    "-stim_times 5 /home/amattfel/Mattfeld_PSB6351/derivatives/first_lvl/sub-021/evs/scenefix_b4_bl_evs.1D \"TWOGAMpw(4,5,0.2,12,7)\" -stim_label 5 scenefx_b4_bl \\\n",
    "-x1D X.task.xmat.1D -xjpeg X.task.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_task = Image.open(os.path.join(os.getcwd(), 'X.task.jpg'))\n",
    "im_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_loc = Image.open(os.path.join(os.getcwd(), 'X.loc.jpg'))\n",
    "im_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
